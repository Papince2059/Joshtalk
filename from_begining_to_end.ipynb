{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2527141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 20 02:56:40 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   45C    P8             14W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45162133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os, shutil\n",
    "from google.colab import drive\n",
    "\n",
    "# If previously mounted, unmount first (safe to ignore errors)\n",
    "try:\n",
    "    drive.flush_and_unmount()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Make mountpoint empty\n",
    "if os.path.exists('/content/drive'):\n",
    "    shutil.rmtree('/content/drive')\n",
    "\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1384a5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script exists: True /content/drive/MyDrive/Josh_Talk/scripts/prepare_joshtalk_hindi.py\n",
      "csv exists: True /content/drive/MyDrive/Josh_Talk/FT_Data - data.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: set paths (update these two)\n",
    "from pathlib import Path\n",
    "\n",
    "SCRIPT_PATH = Path(\"/content/drive/MyDrive/Josh_Talk/scripts/prepare_joshtalk_hindi.py\")\n",
    "CSV_PATH = Path(\"/content/drive/MyDrive/Josh_Talk/FT_Data - data.csv\")\n",
    "\n",
    "OUT_DIR = Path(\"/content/drive/MyDrive/Josh_Talk/data/joshtalk_hindi_hf\")\n",
    "AUDIO_DIR = Path(\"/content/drive/MyDrive/Josh_Talk/data/raw_audio\")\n",
    "CLIPS_DIR = Path(\"/content/drive/MyDrive/Josh_Talk/data/clips\")\n",
    "\n",
    "print(\"script exists:\", SCRIPT_PATH.exists(), SCRIPT_PATH)\n",
    "print(\"csv exists:\", CSV_PATH.exists(), CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d7e1fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: install deps\n",
    "!pip -q install librosa soundfile pandas requests datasets tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1369f1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading + segmenting: 100% 104/104 [01:31<00:00,  1.14it/s]\n",
      "Saving the dataset (1/1 shards): 100% 5214/5214 [00:01<00:00, 5112.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100% 580/580 [00:00<00:00, 645.10 examples/s]\n",
      "{\n",
      "  \"input_rows_hi\": 104,\n",
      "  \"prepared_rows\": 5794,\n",
      "  \"train_rows\": 5214,\n",
      "  \"validation_rows\": 580,\n",
      "  \"failures\": 0,\n",
      "  \"failure_examples\": [],\n",
      "  \"notes\": \"Prepared at segment-level for Whisper (short clips with short text).\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: run preprocessing script\n",
    "!python \"{SCRIPT_PATH}\" \\\n",
    "  --csv_path \"{CSV_PATH}\" \\\n",
    "  --output_dir \"{OUT_DIR}\" \\\n",
    "  --audio_dir \"{AUDIO_DIR}\" \\\n",
    "  --clips_dir \"{CLIPS_DIR}\" \\\n",
    "  --num_workers 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "936eb783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary exists: True\n",
      "{\n",
      "  \"input_rows_hi\": 104,\n",
      "  \"prepared_rows\": 5794,\n",
      "  \"train_rows\": 5214,\n",
      "  \"validation_rows\": 580,\n",
      "  \"failures\": 0,\n",
      "  \"failure_examples\": [],\n",
      "  \"notes\": \"Prepared at segment-level for Whisper (short clips with short text).\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: verify summary\n",
    "import json\n",
    "summary_path = OUT_DIR / \"prepare_summary.json\"\n",
    "print(\"summary exists:\", summary_path.exists())\n",
    "if summary_path.exists():\n",
    "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        print(json.dumps(json.load(f), indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9657cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Josh_Talk/scripts/prepare_joshtalk_hindi.py\n"
     ]
    }
   ],
   "source": [
    "!find /content -type f -name \"prepare_joshtalk_hindi.py\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6361567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script: True /content/drive/MyDrive/Josh_Talk/scripts/prepare_joshtalk_hindi.py\n",
      "csv: True /content/drive/MyDrive/Josh_Talk/FT_Data - data.csv\n",
      "True True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "SCRIPT_PATH = Path(\"/content/drive/MyDrive/Josh_Talk/scripts/prepare_joshtalk_hindi.py\")  # replace with find output\n",
    "CSV_PATH = Path(\"/content/drive/MyDrive/Josh_Talk/FT_Data - data.csv\")\n",
    "\n",
    "print(\"script:\", SCRIPT_PATH.exists(), SCRIPT_PATH)\n",
    "print(\"csv:\", CSV_PATH.exists(), CSV_PATH)\n",
    "\n",
    "\n",
    "OUT_DIR = Path(\"/content/drive/MyDrive/Josh_Talk/data/joshtalk_hindi_hf\")\n",
    "AUDIO_DIR = Path(\"/content/drive/MyDrive/Josh_Talk/data/raw_audio\")\n",
    "CLIPS_DIR = Path(\"/content/drive/MyDrive/Josh_Talk/data/clips\")\n",
    "\n",
    "print(SCRIPT_PATH.exists(), CSV_PATH.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "047cf6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Josh_Talk/scripts/prepare_joshtalk_hindi.py\n",
      "/content/drive/MyDrive/Josh_Talk/FT_Data - data.csv\n"
     ]
    }
   ],
   "source": [
    "# 2) Verify files actually exist in Drive\n",
    "!find \"/content/drive/MyDrive\" -type f -name \"prepare_joshtalk_hindi.py\"\n",
    "!find \"/content/drive/MyDrive\" -type f -name \"FT_Data - data.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb9220a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script: True /content/drive/MyDrive/Josh_Talk/scripts/prepare_joshtalk_hindi.py\n",
      "csv: True /content/drive/MyDrive/Josh_Talk/FT_Data - data.csv\n"
     ]
    }
   ],
   "source": [
    "# 3) Set exact paths from find output\n",
    "from pathlib import Path\n",
    "\n",
    "SCRIPT_PATH = Path(\"/content/drive/MyDrive/Josh_Talk/scripts/prepare_joshtalk_hindi.py\")\n",
    "CSV_PATH = Path(\"/content/drive/MyDrive/Josh_Talk/FT_Data - data.csv\")\n",
    "\n",
    "OUT_DIR = Path(\"/content/drive/MyDrive/Josh_Talk/data/joshtalk_hindi_hf\")\n",
    "AUDIO_DIR = Path(\"/content/drive/MyDrive/Josh_Talk/data/raw_audio\")\n",
    "CLIPS_DIR = Path(\"/content/drive/MyDrive/Josh_Talk/data/clips\")\n",
    "\n",
    "print(\"script:\", SCRIPT_PATH.exists(), SCRIPT_PATH)\n",
    "print(\"csv:\", CSV_PATH.exists(), CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cb3e122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading + segmenting: 100% 104/104 [01:03<00:00,  1.63it/s]\n",
      "Saving the dataset (1/1 shards): 100% 5214/5214 [00:00<00:00, 41281.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100% 580/580 [00:00<00:00, 17368.46 examples/s]\n",
      "{\n",
      "  \"input_rows_hi\": 104,\n",
      "  \"prepared_rows\": 5794,\n",
      "  \"train_rows\": 5214,\n",
      "  \"validation_rows\": 580,\n",
      "  \"failures\": 0,\n",
      "  \"failure_examples\": [],\n",
      "  \"notes\": \"Prepared at segment-level for Whisper (short clips with short text).\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 4) Run only if both are True\n",
    "!pip -q install librosa soundfile pandas requests datasets tqdm\n",
    "!python \"{SCRIPT_PATH}\" \\\n",
    "  --csv_path \"{CSV_PATH}\" \\\n",
    "  --output_dir \"{OUT_DIR}\" \\\n",
    "  --audio_dir \"{AUDIO_DIR}\" \\\n",
    "  --clips_dir \"{CLIPS_DIR}\" \\\n",
    "  --num_workers 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b193830a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary exists: True /content/drive/MyDrive/Josh_Talk/data/joshtalk_hindi_hf/prepare_summary.json\n",
      "{\n",
      "  \"input_rows_hi\": 104,\n",
      "  \"prepared_rows\": 5794,\n",
      "  \"train_rows\": 5214,\n",
      "  \"validation_rows\": 580,\n",
      "  \"failures\": 0,\n",
      "  \"failure_examples\": [],\n",
      "  \"notes\": \"Prepared at segment-level for Whisper (short clips with short text).\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# 5) Check preprocessing summary\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "summary_path = OUT_DIR / \"prepare_summary.json\"\n",
    "print(\"summary exists:\", summary_path.exists(), summary_path)\n",
    "\n",
    "if summary_path.exists():\n",
    "    with open(summary_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        summary = json.load(f)\n",
    "    print(json.dumps(summary, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ca457c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'recording_id', 'user_id', 'language', 'audio', 'sentence', 'start', 'end', 'duration_sec', 'audio_url', 'transcription_url'],\n",
      "        num_rows: 5214\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'recording_id', 'user_id', 'language', 'audio', 'sentence', 'start', 'end', 'duration_sec', 'audio_url', 'transcription_url'],\n",
      "        num_rows: 580\n",
      "    })\n",
      "})\n",
      "train: 5214 validation: 580\n",
      "{'id': '255381_0052', 'recording_id': '255381', 'user_id': '147403', 'language': 'hi', 'audio': '/content/drive/MyDrive/Josh_Talk/data/clips/255381/255381_0052.wav', 'sentence': 'जी, जी जी जी', 'start': 696.36, 'end': 703.98, 'duration_sec': 7.62, 'audio_url': 'https://storage.googleapis.com/upload_goai/362814/255381_audio.wav', 'transcription_url': 'https://storage.googleapis.com/upload_goai/362814/255381_transcription.json'}\n"
     ]
    }
   ],
   "source": [
    "# 6) Quick inspect HF dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(str(OUT_DIR))\n",
    "print(ds)\n",
    "print(\"train:\", len(ds[\"train\"]), \"validation:\", len(ds[\"validation\"]))\n",
    "print(ds[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cdb2f57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m106.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m900.9/900.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m102.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# 2) Install compatible PyTorch stack (CUDA 12.8, matches your torchvision error)\n",
    "!pip -q install --index-url https://download.pytorch.org/whl/cu128 \\\n",
    "  torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1231f518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m104.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# 3) Install NLP/audio training libs\n",
    "!pip -q install -U transformers datasets evaluate jiwer accelerate librosa soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1708e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.0+cu128\n",
      "torchvision: 0.24.0+cu128\n",
      "torchaudio: 2.9.0+cu128\n",
      "cuda available: True\n"
     ]
    }
   ],
   "source": [
    "# 4) Verify\n",
    "import torch, torchvision, torchaudio\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"torchvision:\", torchvision.__version__)\n",
    "print(\"torchaudio:\", torchaudio.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb3be17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'recording_id', 'user_id', 'language', 'audio', 'sentence', 'start', 'end', 'duration_sec', 'audio_url', 'transcription_url'],\n",
      "        num_rows: 5214\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'recording_id', 'user_id', 'language', 'audio', 'sentence', 'start', 'end', 'duration_sec', 'audio_url', 'transcription_url'],\n",
      "        num_rows: 580\n",
      "    })\n",
      "})\n",
      "dict_keys(['id', 'recording_id', 'user_id', 'language', 'audio', 'sentence', 'start', 'end', 'duration_sec', 'audio_url', 'transcription_url'])\n"
     ]
    }
   ],
   "source": [
    "# Cell B: load dataset + cast audio column\n",
    "from datasets import load_from_disk, Audio\n",
    "\n",
    "ds = load_from_disk(str(OUT_DIR))\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "print(ds)\n",
    "print(ds[\"train\"][0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f724a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89f93870549a4588bfa91dd4b9d8dae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe5f7afd4e14ca3b1130c21bd6ec654",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52c9cb37d584481bc9f2cb66463661e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e53c40bb3db40adac6ea7e0a63b3357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4779a9811e774752aeaa986c2a29dd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be36e3c5cff406caa9321937f1f3d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3632ba7207a04f99b6dcffdc90a309cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfb48e1eaf24132a632380306ea895d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e267a746a03946ff87bfbbc1bfb3fdd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40329619fd074e60b9f5b01aa053e884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb41009a83de43aa88f357205c0ab376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5781335e83d24eb1966891e0bcb5b95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell C: processor/model setup\n",
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "MODEL_ID = \"openai/whisper-small\"   # use \"openai/whisper-base\" if GPU is small\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_ID, language=\"Hindi\", task=\"transcribe\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Keep generation behavior in generation_config only.\n",
    "forced_ids = processor.get_decoder_prompt_ids(language=\"hindi\", task=\"transcribe\")\n",
    "model.generation_config.forced_decoder_ids = forced_ids\n",
    "model.generation_config.suppress_tokens = []\n",
    "model.generation_config.language = \"hindi\"\n",
    "model.generation_config.task = \"transcribe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30f9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell D: preprocess dataset\n",
    "\n",
    "def prepare_batch(batch):\n",
    "    audio = batch[\"audio\"]\n",
    "    feats = processor.feature_extractor(\n",
    "        audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "    batch[\"input_features\"] = feats[\"input_features\"][0]\n",
    "    batch[\"attention_mask\"] = feats[\"attention_mask\"][0]\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "    return batch\n",
    "\n",
    "\n",
    "cols_to_remove = ds[\"train\"].column_names\n",
    "ds_proc = ds.map(\n",
    "    prepare_batch,\n",
    "    remove_columns=cols_to_remove,\n",
    "    num_proc=1,  # keep 1 in colab for stability\n",
    ")\n",
    "print(ds_proc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843874f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After Cell D (run once)\n",
    "PROC_DIR = \"/content/drive/MyDrive/Josh_Talk/data/joshtalk_hindi_hf_proc\"\n",
    "ds_proc.save_to_disk(PROC_DIR)\n",
    "print(\"saved:\", PROC_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d9ec5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 5214\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_features', 'labels'],\n",
      "        num_rows: 580\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# In future sessions (instead of rerunning map)\n",
    "from datasets import load_from_disk\n",
    "PROC_DIR = \"/content/drive/MyDrive/Josh_Talk/data/joshtalk_hindi_hf_proc\"\n",
    "ds_proc = load_from_disk(PROC_DIR)\n",
    "print(ds_proc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eff344c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74e4e28bbc24d0ba652e804d070d0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49f9d66335443da89bb7a805efbc642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell E: data collator + metrics (patched)\n",
    "import torch\n",
    "import jiwer\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "wer_metric = jiwer.wer\n",
    "cer_metric = jiwer.cer\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
    "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "\n",
    "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "\n",
    "        # remove BOS if present in all labels\n",
    "        bos_id = self.processor.tokenizer.bos_token_id\n",
    "        if bos_id is not None and labels.size(1) > 0 and (labels[:, 0] == bos_id).all().item():\n",
    "            labels = labels[:, 1:]\n",
    "\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n",
    "\n",
    "\n",
    "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    # some versions return tuple(pred_ids, ...)\n",
    "    if isinstance(pred_ids, tuple):\n",
    "        pred_ids = pred_ids[0]\n",
    "\n",
    "    label_ids = pred.label_ids.copy()\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * wer_metric(label_str, pred_str)\n",
    "    cer = 100 * cer_metric(label_str, pred_str)\n",
    "    return {\"wer\": wer, \"cer\": cer}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "464ff78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix generation config for newer transformers\n",
    "forced_ids = processor.get_decoder_prompt_ids(language=\"hindi\", task=\"transcribe\")\n",
    "\n",
    "model.generation_config.forced_decoder_ids = forced_ids\n",
    "model.generation_config.suppress_tokens = []\n",
    "model.generation_config.language = \"hindi\"\n",
    "model.generation_config.task = \"transcribe\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a542463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available(), torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "432823c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 20 03:06:46 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   46C    P8             14W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "cuda: True count: 1\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(\"cuda:\", torch.cuda.is_available(), \"count:\", torch.cuda.device_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26e04801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available(), torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.cuda.is_available(), \"GPU not available. Switch Colab runtime to GPU before training.\"\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10e2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint: /content/drive/MyDrive/Josh_Talk/whisper_hi_small_run/checkpoint-400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ab522b557245f7ab96f4805aa5ce44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/479 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  51/2000 05:23 < 3:34:44, 0.15 it/s, Epoch 0.15/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "RUN_DIR = \"/content/drive/MyDrive/Josh_Talk/whisper_hi_small_run\"\n",
    "Path(RUN_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ckpts = sorted(Path(RUN_DIR).glob(\"checkpoint-*\"), key=lambda p: int(p.name.split(\"-\")[-1]))\n",
    "resume_ckpt = str(ckpts[-1]) if ckpts else None\n",
    "print(\"Latest checkpoint:\", resume_ckpt)\n",
    "\n",
    "forced_ids = processor.get_decoder_prompt_ids(language=\"hindi\", task=\"transcribe\")\n",
    "model.generation_config.forced_decoder_ids = forced_ids\n",
    "model.generation_config.suppress_tokens = []\n",
    "model.generation_config.language = \"hindi\"\n",
    "model.generation_config.task = \"transcribe\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=RUN_DIR,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=200,\n",
    "    max_steps=2000,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=225,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"wer\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=ds_proc[\"train\"],\n",
    "    eval_dataset=ds_proc[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "train_result = trainer.train(resume_from_checkpoint=resume_ckpt if resume_ckpt else None)\n",
    "print(\"Training done. Global step:\", trainer.state.global_step)\n",
    "\n",
    "# Persist final model + processor in run directory.\n",
    "trainer.save_model(RUN_DIR)\n",
    "processor.save_pretrained(RUN_DIR)\n",
    "\n",
    "metadata = {\n",
    "    \"latest_checkpoint_used\": resume_ckpt,\n",
    "    \"best_checkpoint\": trainer.state.best_model_checkpoint,\n",
    "    \"global_step\": int(trainer.state.global_step),\n",
    "    \"train_runtime\": train_result.metrics.get(\"train_runtime\") if hasattr(train_result, \"metrics\") else None,\n",
    "}\n",
    "Path(RUN_DIR, \"run_metadata.json\").write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved final artifacts to:\", RUN_DIR)\n",
    "print(\"Metadata file:\", Path(RUN_DIR, \"run_metadata.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime-first rule + VS Code reconnect fallback\n",
    "print('For long runs: keep the Colab browser tab connected and run training there.')\n",
    "print('Use VS Code mainly for editing. Colab remote URLs are temporary and expire.')\n",
    "print('Reconnect steps:')\n",
    "print('1) In Colab browser, reconnect runtime and get a fresh Jupyter URL.')\n",
    "print('2) In VS Code: Jupyter: Clear Jupyter Server MRU.')\n",
    "print('3) VS Code: Jupyter: Specify Jupyter Server for Connections -> Existing -> paste fresh URL.')\n",
    "print('4) Re-select kernel from that server.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f465fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell G: final evaluation (safe + explicit)\n",
    "best_ckpt = trainer.state.best_model_checkpoint\n",
    "print(\"Best checkpoint:\", best_ckpt)\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bcc3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell H: save final model + processor (save best checkpoint if available)\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "SAVE_DIR = \"/content/drive/MyDrive/Josh_Talk/whisper_hi_small_final\"\n",
    "\n",
    "if best_ckpt is not None:\n",
    "    # Copy best checkpoint files as final export\n",
    "    shutil.copytree(best_ckpt, SAVE_DIR, dirs_exist_ok=True)\n",
    "    # Ensure processor files are also present\n",
    "    processor.save_pretrained(SAVE_DIR)\n",
    "else:\n",
    "    # Fallback: save current in-memory model\n",
    "    trainer.save_model(SAVE_DIR)\n",
    "    processor.save_pretrained(SAVE_DIR)\n",
    "\n",
    "print(\"Saved to:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final: FLEURS Hindi test evaluation (baseline vs fine-tuned)\n",
    "import pandas as pd\n",
    "import torch\n",
    "import jiwer\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "wer_metric = jiwer.wer\n",
    "fleurs = load_dataset(\"google/fleurs\", \"hi_in\")\n",
    "test_ds = fleurs[\"test\"].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "def eval_model(model_id_or_path, label):\n",
    "    proc = WhisperProcessor.from_pretrained(model_id_or_path, language=\"Hindi\", task=\"transcribe\")\n",
    "    mdl = WhisperForConditionalGeneration.from_pretrained(model_id_or_path).to(\n",
    "        \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    forced_ids = proc.get_decoder_prompt_ids(language=\"hindi\", task=\"transcribe\")\n",
    "\n",
    "    preds, refs = [], []\n",
    "    for ex in test_ds:\n",
    "        inputs = proc(ex[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\").to(mdl.device)\n",
    "        with torch.no_grad():\n",
    "            out = mdl.generate(**inputs, forced_decoder_ids=forced_ids, max_new_tokens=128)\n",
    "        preds.append(proc.batch_decode(out, skip_special_tokens=True)[0])\n",
    "        refs.append(ex[\"transcription\"])\n",
    "\n",
    "    wer = wer_metric(refs, preds)\n",
    "    return {\"Model\": label, \"WER\": wer, \"Pseudo_Accuracy_(1-WER)\": 1 - wer}\n",
    "\n",
    "results = [\n",
    "    eval_model(\"openai/whisper-small\", \"Whisper-small (pretrained)\"),\n",
    "    eval_model(\"/content/drive/MyDrive/Josh_Talk/whisper_hi_small_final\", \"Whisper-small (fine-tuned)\")\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.to_string(index=False))\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f729349f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell I: quick inference test (Hindi transcription)\n",
    "import torch\n",
    "\n",
    "sample = ds[\"validation\"][0]\n",
    "inputs = processor(\n",
    "    sample[\"audio\"][\"array\"],\n",
    "    sampling_rate=16000,\n",
    "    return_tensors=\"pt\",\n",
    "    return_attention_mask=True,\n",
    ").to(model.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "\n",
    "pred_text = processor.batch_decode(pred_ids, skip_special_tokens=True)[0]\n",
    "print(\"REF :\", sample[\"sentence\"])\n",
    "print(\"PRED:\", pred_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
